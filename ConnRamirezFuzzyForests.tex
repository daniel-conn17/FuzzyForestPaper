\documentclass[article,shortnames]{jss}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% declarations for jss.cls %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{amsmath}                
\usepackage{amssymb}
\usepackage{amsthm}



%% almost as usual
\author{Daniel Conn\\UCLA SPH, Biostatistics \And Tuck Ngun \And Christina Ramirez\\UCLA SPH, Biostatistics }
\title{Fuzzy Forests: A New WGCNA Based Random Forest Algorithm for Correlated, High-Dimensional Data}

%% for pretty printing and a nice hypersummary also set:
\Plainauthor{Daniel Conn, Christina Ramirez} %% comma-separated
\Plaintitle{Fuzzy Forests: A New WGCNA Based Random Forest Algorithm for High-Dimensional, Correlated Data} %% without formatting
\Shorttitle{\pkg{fuzzyforest}: Fuzzy Forests in \proglang{R} } %% a short title (if necessary)

%% an abstract and keywords
\Abstract{
 In this paper we introduce fuzzy forests, a new machine learning algorithm for ranking the importance of features in high-dimensional classification and regression
 problems.  Fuzzy forests is specifically designed to provide unbiased rankings of variable importance in the presence of correlated features. 
Fuzzy forests uses Weighted Gene Coexpressionn Network Analysis (WGCNA) to detect groups of highly correlated features.  Unbiased rankings are
obtained by fitting separate random forests on each module.  We also introduce our implementation of fuzzy forests in the \proglang{R} package, \pkg{fuzzyforests}.     
}
\Keywords{Random Forests, WGCNA, machine learning,\proglang{R}}
\Plainkeywords{Random Forests, WGCNA, machine learning,R} %% without formatting
%% at least one keyword must be supplied

%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{50}
%% \Issue{9}
%% \Month{June}
%% \Year{2012}
%% \Submitdate{2012-06-04}
%% \Acceptdate{2012-06-04}

%% The address of (at least) one author should be given
%% in the following format:
\Address{
  Daniel Conn\\
  Department of Biostatistics\\
  UCLA School of Public Health\\
  Los Angeles, United States of America\\
  E-mail: \email{djconn17@gmail.com}\\
}
%% It is also possible to add a telephone and fax number
%% before the e-mail in the following format:
%% Telephone: +43/512/507-7103
%% Fax: +43/512/507-2851

%% for those who use Sweave please include the following line (with % symbols):
%% need no \usepackage{Sweave.sty}

%% end of declarations %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}

%% include your article here, just as usual
%% Note that you should use the \pkg{}, \proglang{} and \code{} commands.

\section[Introduction]{Introduction}
%% Note: If there is markup in \(sub)section, then it has to be escape as above.
The problem of identifying important features in the presence of correlation has been an area of intense research within the statistics
and machine learning community.  Biological applications have, in particular, spurred the development of high dimensional feature selection methods.   
While model based feature selection algorithms such as the LASSO or SCAD may efficiently detect important features in presence of correlation \citep{raskutti2010restricted}, this efficiency comes at the cost of making parametric assumptions that may not hold in practice. 
 
Random forest variable importance measures (VIMs) offer a nonparametric alternative to model based feature selection algorithms \citep{breiman2001random}.
Random forests is a popular ensemble based machine learning algorithm.  While random forest VIMs have demonstrated the ability to accurately capture the true 
importance of features in settings where the features are independent, it is well-known that random forests VIMs are biased when features are correlated
with one another (\cite{strobl2007bias}; \cite{strobl2008conditional}; \cite{nicodemus2009predictor}).

Fuzzy forests cope with correlated features by taking a piecewise approach.  We partition the set of features into distinct modules such that the correlation within each module is high and the correlation between groups is low.  We then use an iterative feature selection algorithm to select the most important
features from each.  A random forest is then fit to the features that have survived this first round.  A final iterative random forest,  combining features from all
modules, selects the top $k$ features. 

There are a variety of algorithms for partitioning features into distinct, high correlation modules.  In this regard, WGCNA is our method of choice.  WGCNA is a rigorous framework for detecting correlation networks \citep{zhang2005general}.  Although it was first developed to detect modules of highly correlated genes, it has found application in a variety of biological contexts.  The \proglang{R} package \pkg{WGCNA} is a robust, computationally efficient, and well-documented implementation of the WGCNA framework.  We expect that researchers already familiar with \pkg{WGCNA} will easily adopt the fuzzy forests algorithm and we expect that newcomers to WGCNA will be able to make good use of \pkg{WGCNA}'s fine documentation and tutorials.
              
The article is organized as follows.  In section 2 of this article, we briefly review the random forests, WGCNA, and introduce the fuzzy forests algorithm.
In section 3, we introduce the \proglang{R} package \pkg{fuzzyforest}.  In section 4, we provide a heuristic proof that under the right assumptions, the VIMs obtained by fitting a separate random forest on each module are asymptotically unbiased.  In section 5, we conduct simulations to comparing fuzzy forests to both random forests and conditional inference forests (CIFs).  We demonstrate that fuzzy forests has performance comparable to that of CIF's although at much lower computational cost.  In section 6, we use fuzzy forests to determine which biological factors are important in determining how well an HIV patient copes with the virus.  Section 7 ends the article with a discussion and summary of our results. 

\section[Fuzzy Forests Algorithm]{Variable Importance Measures and the Fuzzy Forests Algorithm}
\subsection{Variable Importance Measures}[2.1]
In this section, we introduce basic notation and define variable importance measures.  We assume that our data comes in the form of $n$ independently and identically distributed iid.
pairs $(X,Y) \sim G$.  Here $X$ is $p$ dimensional feature vector and $Y$ is a scalar outcome.  The value of the $v$th feature for the the $i$th subject will be denoted by $X_{i}^{(v)}$.
The feature vector for the $i$th subject is denoted by $X_{i}=(X_{i}^{(1)},\ldots,X_{i}^{(p)})$.   Finally, let $X^{(v)}=(X_{1}^{(v)},\ldots,X_{n}^{(v)})$ be set of values for feature $v$ across
all $n$ subjects.

In the case of both classification and regression we are interested in modeling the conditional mean of $Y$ given a feature vector $X_{i}$.  We denote this conditional mean alternatively as $E[Y|X]$ or $f(X)$ and we assume that $Y|X$ has distribution $f(X) + \epsilon$, where the $\epsilon$ are $iid$ with variance $\sigma^{2}$.   In the regression setting, $Y$ is unrestricted. 
In classification, $Y$ is restricted to take the value 0 or 1.  

If the goal is to predict a new outcome $Y$ based off of measurements, $X$, a good estimate of $f(X)$ is all that is required.
We are interested in more than prediction.  We are interested in understanding how $f(X)$ changes as function of particular features. 
If the value of $f(X)$ varies widely according to a particular value of the $v$th feature, $X^{(v)}$ is, in some sense, an ``important" in determinant of
the outcome, $Y$.  

If $p$ is low dimensional $(p=1,2)$, we can simply plot our estimate of $f(X)$ to understand how it varies as function of $X$. 
If $p$ is moderate or large, $f(X)$ is difficult to interpret.  It is most common in this case, to assume $f(X)$ has a specific parametric
form so that $f_{\beta}(X)$ is known up to a finite dimensional parameter $\beta$.  In the case of linear regression, $\beta$ is a vector of
regression coefficients and we can measure the importance of one feature versus another feature by examining the absolute magnitude 
of their corresponding coefficents.    

If the parametric model $f_{\beta}(X)$ is a close approximation to $f(X)$, it is possible that interpretations based off of $\hat{f}_{\hat{\beta}}(X)$
will not be misleading.  Likewise, if $f_{\beta}(X)$ is a poor approximation of $f(X)$, the resulting interpretation will be misleading.  
The parametric approximation $f_{\beta}(X)$ may be inadequate for a variety of reasons.  This may occur if important features are 
not observed.  Even if all appropriate features are measured, $f_{\beta}(X)$ may fail to capture important interactions between features.
If $f_{\beta}(X)$ is a linear regression model, $\sum_{v=1}^{p}\beta_{v}X^{(v)}$, the true $f(X)$ may be nonlinear in such a way that this best linear approximation fails to capture.
  
Permutation VIMs provide a means of summarizing the importance of individual features without making parametric assumptions.
We define the permutation VIM of feature $v$, $X^{(v)}$, as 
\begin{equation}
VIM(v)=E(f(X^{(1)}_{i},\ldots,X^{(v)}_{i},\ldots,X^{(p)}_{i}) - f(X^{(1)}_{i},\ldots,\tilde{X}^{(v)}_{i},\ldots,X^{(p)}_{i}))^{2}.
\end{equation}
Here, $X^{(v)}_{i}$ and $\tilde{X}^{(v)}_{i}$ are iid with distribution $G_{X^{(v)}}$ where $G_{X^{(v)}}$ is the marginal distribution of $X^{(v)}_{i}$.  This form of the VIM
is given in a slightly different form in \citep{gregorutti2013correlation} and \citep{zhu2012reinforcement}.  These authors also discuss conditions under
which the estimate of the permutation VIM derived from random forests is consistent. 

         
\subsection{An Introduction to Random Forests}[2.2]
Random forests is a popular ensemble method that has been applied in the setting of both classification and regression.  The random forests algorithm works
by combining the predictions of an ensemble of classification and regression trees.  Each tree is grown on a separate bootstrap sample of the data.  The number
of trees grown in this manner is denoted as $ntree$.  The subjects that are not selected in a particular bootstrap sample are said to be ``out of bag."  

Call the $k$th tree $\hat{f}_{k}(X)$.  In the case of regression trees,
$\hat{f}(X)=\frac{1}{ntree}\sum_{k=1}^{ntree}\hat{f}_{k}(X)$.  In the case of classification, $\hat{f}(X)$ is the majority vote of the $ntree$ predictions given
by $\hat{f}_{k}(X)$.  Each regression tree is highly unstable and gives highly variable predictions.  Averaging multiple trees over
many bootstrap samples leads to more stable estimates of $f(X)$.  The algorithm described thus far is known as bagging (bootstrap-aggregating).  This 
algorithm is a special case of random forests.  

A further element of randomness is introduced by random forests.  Before a node in a particular tree is split, a subset of features is chosen at random.  Of these randomly 
chosen features, the feature with the highest marginal importance is used to split the node.  The number of randomly selected features at each stage is
commonly denoted as $mtry$.  High values of $mtry$ tend to lead to just a few important features getting selected at the majority cut-points.
Lower values of $mtry$ allow more features to play a role in the estimation $f(X)$.  In the case of regression, a common default value of $mtry$ is $\sqrt{p}$.
In the case of classification  $\left\lfloor p/3 \right\rfloor$ is common choice.
  
Random forest VIMs are obtained by testing how predictive accuracy suffers when the values of individual predictors are permuted.  Let $OOB_{k} \subset \{1,\ldots, n\}$ 
be the out of bag samples in the $k$th bootstrap sample.  Let $\pi_{k}$ be a random permutation of the elements of $OOB_{k}$ and let   
$\pi_{k}^{(v)}(X_{i}) = (X_{i}^{(1)},\ldots,X_{\pi_{k}(i)}^{(v)},\ldots,X_{i}^{(p)}),$ where $i \in OOB_{k}$.   In other words, $\pi_{k}^{(v)}$ 
permutes the values of the $v$th feature across all out of bag subjects.  The variable importance of the $i$th feature from the $k$th tree is defined as
\begin{equation}
\widehat{VIM}^{k}(v)= \frac{\sum_{i \in OOB_{k}}(y_{i}-\hat{f}^{k}(\pi_{k}^{(v)}(X_{i}))^{2} - (y_{i} - \hat{f}^{k}(X_{i}))^{2}}{|OOB_{k}|}
\end{equation}
The variable importance for the entire random forest is defined as
\begin{equation}
\widehat{VIM}(v) = \frac{\sum_{k=1}^{ntree}\widehat{VIM}^{k}(v)}{ntree}
\end{equation}

\subsection{A Brief Review of WGCNA}[2.3]
WGCNA is a rigorous framework for constructing a network of features. 
This network is then used to determine clusters or modules of inter-related features. 
We briefly review the steps of a WGCNA network analysis.  The user first specifies a similarity function $s_{ij}=S(X^{(i)},X^{(j)})$ taking
values between 0 and 1.  Both unsigned and signed networks are possible.  If the features are continuous, the most common choice 
of similarity function is $|Corr(X^{(i)},X^{(j)})|$ or $\frac{1 + Corr(X^{(i)},X^{(j)})}{2}$  according to whether the network is unsigned or 
signed.

This similarity matrix is then transformed into an adjacency matrix $A=[a_{ij}]$.  This adjacency function determines how the adjacency function
translates into network adjacencies.  The simplest choice of adjacency function is the $signum(s_{ij},\tau)$ function.  This function
simply sets a hard threshold $\tau$.  If $s_{ij} \geq \tau, a_{ij}=signum(s_{ij},\tau)=1$ otherwise $a_{ij}=0$.  Nodes are either connected or 
un-connected if the $signum$ adjacency function is used.  In practice, a soft-thresholded network is often more plausible than a 
hard-thresholded one.  The power function $a_{ij}=s_{ij}^{q}$ is common choice of soft-thresholding adjacency function.  Large values
of $q$ yield behavior closer to a hard-thresholded network.  Once an adjacency function is calculated, an hierarchical clustering tree algorithm 
may be used to define clusters of features.

It is common to apply this hierarchical clustering algorithm to the topological overlap matrix rather than the adjacency matrix.  The topological
overlap between two nodes is defined as 
\begin{equation}
\omega_{ij} = \frac{l_{ij} + a_{ij}}{min\{k_{i},k_{j}\} + 1 - a_{ij}}
\end{equation} 
where $l_{ij}=\sum_{u}a_{iu}a_{uj}$ and $k_{i}=\sum_{u}a_{iu}$.  The topological overlap between two nodes can be high even if $a_{ij}$ is low.
This occurs when the two nodes are strongly connected to the same set of nodes.  Use of topological overlap rather
than the adjacencies may lead to more distinct modules.

In many biological contexts, it is suspected that only a few features are highly connected.  This prior knowledge leads to the scale-free criterion for
determining which value of $q$ to select.  A network is said to have a generalized scale-free topology if $p(k) \sim k^{\gamma}$ or
 $\log_{10}(p(k)) \propto \log_{10}(k)$.  This suggests that one should select a smaller value of $q$ such that the $R^{2}$ between 
  $\log_{10}(p(k))$ and $\log_{10}(k)$ is high.
  
  \subsection{The Fuzzy Forests Algorithm}[2.4]


\bibliography{RFrefs}
\end{document}





